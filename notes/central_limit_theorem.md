# Central Limit Theorem

Given any probability distribution that has a well-defined mean and variance, continuous or discrete, and not necessarily a normal distribution, the greater the sample size n, the more normal the frequency distribution is for the sample mean and the sample sum. A sample mean is the mean of the values of n random samples; the set of these samples make up a random sample.

As n approaches infinity, the more normal is the curve of the sampling distribution.
- There is a tighter fit around the mean. The standard deviation is reduced.
- Skew is closer to 0.
- Kurtosis is closer to 0. (Positive kurtosis has a curve that is pointy in the middle and with fatter tails; negative kurtosis has skinnier tails and a smoother hump.)

You don't need to know the probability distribution of the original population. CLT says that if you sample it enough for sample means or sample sums, you will get a normal distribution. This makes the CLT useful for describing natural processes with random behavior.

CLT shows that the sample mean is equal to the population mean.

> When a population is not normally distributed, the sampling distribution of 
x̄ depends on the sample size. For smaller samples (n<30), the shape of the sampling distribution of the sample mean x̄ will match the shape of the parent population, but it will be closer to normal than the parent population is. For larger samples (n >= 30), the central limit theorem tells us that the sampling distribution of the sample mean x̄ will be approximately normal regardless of the shape of the parent population. (From https://www.khanacademy.org/math/ap-statistics/sampling-distribution-ap/what-is-sampling-distribution/e/sample-means-central-limit-theorem)
